train_x <- train[,2:10]
intercept <- rep(1, nrow(train_x))
train_x <- cbind(intercept,train_x)
train_y <- train[,11]
train_y
theta <- grad.descent(x = train_x,y = train_y,alpha = 0.001)
theta
dim(y)
dim(x)
dim(theta)
y = train_y
x = train_x
alpha = 0.001
theta = runif(ncol(x)) # Initialize the parameters
for (i in 1:maxit) {
thetaOld <- theta
gradient <- gcost(x, y, theta)
print(gradient)
theta <- thetaOld - alpha  * gradient
#print(gradient)
if ( delta > max(abs(theta - thetaOld)) ){
print(paste("iteration",i));
print(theta)
return(theta)
}
}
gcost<- function(x, y, theta) {
gradient <-  y %*% as.matrix(x)/ ( 1 +exp(-y %*% as.matrix(x) *t(theta)))
return(t(gradient))
}
theta = runif(ncol(x)) # Initialize the parameters
#alpha = .05 # set learning rate
for (i in 1:maxit) {
thetaOld <- theta
gradient <- gcost(x, y, theta)
print(gradient)
theta <- thetaOld - alpha  * gradient
#print(gradient)
if ( delta > max(abs(theta - thetaOld)) ){
print(paste("iteration",i));
print(theta)
return(theta)
}
}
return(theta)
intercept
x
x*theta
x%*%theta
x%*%t(theta)
dim(x)
dim(theta)
x%*%ttheta
x%*%theta
as.matrix(x)%*%as.matrix(theta)
data(BreastCancer)
library(mlbench)
data(BreastCancer)
BreastCancer <- BreastCancer[complete.cases(BreastCancer),]
BreastCancer[,2:10] <- lapply(BreastCancer[,2:10], function (x) as.numeric(levels(x))[x])
BreastCancer[,11] <- as.numeric(BreastCancer[,11])
View(BreastCancer)
data(BreastCancer)
df <- BreastCancer[complete.cases(BreastCancer),]
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df[,11] <- as.numeric(df[,11])-1
smp_size <- floor(0.70 * nrow(BreastCancer))
View(df)
set.seed(111)
train_ind <- sample(seq_len(nrow(BreastCancer)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
intercept <- rep(1, nrow(train_x))
train_x <- cbind(intercept,train_x)
train_y <- train[,11]
intercept <- rep(1, nrow(train_x))
train_x <- cbind(intercept,train_x)
theta <- grad.descent(x = train_x,y = train_y,alpha = 0.001)
x = train_x
y = train_y
maxit = 10000
alpha = 0.5
delta = 1e-8
gradient <-  y %*% as.matrix(x)/ ( 1 +exp(-y %*% as.matrix(x) *t(theta)))
data(BreastCancer)
# remove NA
df <- BreastCancer[complete.cases(BreastCancer),]
# data type convert for variables other than `ID` and `Class`
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df[,11] <- as.numeric(df[,11])-1
smp_size <- floor(0.70 * nrow(BreastCancer))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
train_y <- train[,11]
intercept <- rep(1, nrow(train_x))
train_x <- cbind(intercept,train_x)
1 +exp(-y %*% as.matrix(x) *t(theta)))
1 +exp(-y %*% as.matrix(x) *t(theta))
theta = runif(ncol(x)) # Initialize the parameters
1 +exp(-y %*% as.matrix(x) *t(theta))
1 +exp(-y %*% as.matrix(x) *t(theta))
theta
1 +exp(-y %*% as.matrix(x) *t(theta))x
x
1 +exp(-y %*% as.matrix(x) *t(theta))x
is.na(x)
sum(is.na(x))
data(BreastCancer)
df <- BreastCancer[complete.cases(BreastCancer),]
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df[,11] <- as.numeric(df[,11])-1
smp_size <- floor(0.70 * nrow(df))
set.seed(111)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
train_y <- train[,11]
is.na(train_x)
sum(is.na(train_x))
theta
theta <- grad.descent(x = train_x,y = train_y,alpha = 0.001)
intercept <- rep(1, nrow(train_x))
intercept
train_x <- cbind(intercept,train_x)
alpha = 0.05
theta = runif(ncol(x)) # Initialize the parameters
thetaOld <- theta
gradient <- gcost(x, y, theta)
gradient <-  y %*% as.matrix(x)/ ( 1 +exp(-y %*% as.matrix(x) *t(theta)))
gradient
sum(is.na(as.matrix(X)))
sum(is.na(as.matrix(x)))
theta
data(BreastCancer)
# data type convert for variables other than `ID` and `Class`
df <- BreastCancer[complete.cases(BreastCancer),]
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df[,11] <- as.numeric(df[,11])-1
train_x <- train[,2:10]
train_y <- train[,11]
intercept <- rep(1, nrow(train_x))
train_x <- cbind(intercept,train_x)
x = train_x
y = train_y
alpha = 0.001
theta = runif(ncol(x))
thetaOld <- theta
gradient <- gcost(x, y, theta)
gcost<- function(x, y, theta) {
gradient <-  y %*% as.matrix(x)/ ( 1 +exp(-y %*% as.matrix(x) *t(theta)))
return(t(gradient))
}
gradient <- gcost(x, y, theta)
class(gradient)
theta <- thetaOld - alpha  * gradient
return(theta)
y_hat <- as.matrix(test[,2:10]) %*% theta[2:10]
y_hat
as.matrix(x) %*% t(theta)
theta
as.matrix(x) %*% theta
y %*%( as.matrix(x) %*% theta))
y %*%( as.matrix(x) %*% theta)
exp(-y %*%( as.matrix(x) %*% theta))
exp(4759)
exp(4759)+1
-y %*%( as.matrix(x) %*% theta))
-y %*%( as.matrix(x) %*% theta)
y %*% as.matrix(x
)
y
( 1 + exp(-y %*%( as.matrix(x) %*% theta)))
1/( 1 + exp(-y %*%( as.matrix(x) %*% theta)))
-y *( as.matrix(x) %*% theta
)
exp(-y *( as.matrix(x) %*% theta))
1 + exp(-y *( as.matrix(x) %*% theta))
y %*% as.matrix(x)/ ( 1 + exp(-y *( as.matrix(x) %*% theta)))
y %*% as.matrix(x)
theta %*% x
x
x
theta %*% x
x %*% theta
y / ( 1 + exp(-y *( as.matrix(x) %*% theta)))
dim(y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
dim(x)
gcost<- function(x, y, theta) {
gradient <- t(as.matrix(x)) (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
return(t(gradient))
}
gradient <- gcost(x, y, theta)
t(as.matrix(x)) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
dim(x)
dim(theta)
dim(y)
temp <-as.matrix(x) %*% theta
class(y)
class(temp)
y*temp
df <- BreastCancer[complete.cases(BreastCancer),]
# data type convert for variables other than `ID` and `Class`
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df[,11] <- as.numeric(df[,11])-1
smp_size <- floor(0.70 * nrow(df))
# set the seed to make your partition reproductible
set.seed(111)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
train_y <- train[,11]
intercept <- rep(1, nrow(train_x))
train_x <- cbind(intercept,train_x)
x = train_x
y = train_y
alpha = 0.001
maxit = 20000
delta = 1e-8
theta = runif(ncol(x)) # Initialize the parameters
thetaOld <- theta
gradient <- gcost(x, y, theta)
gcost<- function(x, y, theta) {
gradient <- as.matrix(x) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
return(t(gradient))
}
gradient <- gcost(x, y, theta)
gradient <- as.matrix(x) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
exp(-y *( as.matrix(x) %*% theta))
y / ( 1 + exp(-y *( as.matrix(x) %*% theta)))
as.matrix(x) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
dim(y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
dim(x)
dim(t(x))
t(as.matrix(x)) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
gradient <- gcost(x, y, theta)
gcost<- function(x, y, theta) {
gradient <- t(as.matrix(x)) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
return(t(gradient))
}
gradient <- gcost(x, y, theta)
print(paste("iteration",i));
theta <- thetaOld - alpha  * gradient
theta
class((y / ( 1 + exp(-y *( as.matrix(x) %*% theta)))))
class(( ( 1 + exp(-y *( as.matrix(x) %*% theta)))))
class(( ( exp(-y *( as.matrix(x) %*% theta)))))
class(( ( exp(( as.matrix(x) %*% theta)))))
class(( ( exp(( as.matrix(x) %*% as.vector(theta)))))
)
if ( delta > max(abs(theta - thetaOld)) ){
print(paste("iteration",i));
print(theta)
return(theta)
}
thetaOld <- theta
gradient <- gcost(x, y, theta)
theta <- thetaOld - alpha  * gradient
theta
#print(gradient)
if ( delta > max(abs(theta - thetaOld)) ){
print(paste("iteration",i));
print(theta)
return(theta)
}
thetaOld <- theta
gradient <- gcost(x, y, theta)
dim(x)
dim(theta)
dim(gradient)
gcost<- function(x, y, theta) {
theta <- as.vector(theta)
gradient <- t(as.matrix(x)) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
return(gradient)
}
gradient <- gcost(x, y, theta)
theta <- thetaOld - alpha  * gradient
dim(thetaOld)
dim(gradient)
set.seed(1441)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
train_y <- train[,11]
intercept <- rep(1, nrow(train_x))
train_x <- cbind(intercept,train_x)
x = train_x
y = train_y
alpha = 0.001
theta = runif(ncol(x)) # Initialize the parameters
dim(theta)
dim(as.vector(theta))
class(theta)
theta
thetaOld <- theta
gradient <- gcost(x, y, theta)
theta <- thetaOld - alpha  * gradient
thetaOld <- theta
gradient <- gcost(x, y, theta)
theta <- thetaOld - alpha  * gradient
theta
alpha = 0.05
thetaOld <- theta
gradient <- gcost(x, y, theta)
theta <- thetaOld - alpha  * gradient
theta
gradient
dim(gray())
dim(gradient)
dim(theta)
max(gradient)
data(BreastCancer)
# remove NA
df <- BreastCancer[complete.cases(BreastCancer),]
# data type convert for variables other than `ID` and `Class`
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df$label = rep(-1, nrow(copy)) # create new column for label
df$label = rep(-1, nrow(df)) # create new column for label
df[df$Class=="malignant",]$label = 1 # malignant is 1, benign is -1
smp_size <- floor(0.70 * nrow(df))
set.seed(1441)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
train_y <- train[,12]
train_x$intercept <- rep(1, nrow(train_x))
x = train_x
y = train_y
alpha = 0.001
theta = runif(ncol(x)) # Initialize the parameters
alpha <- alpha * 0.98
thetaOld <- theta
gradient <- gcost(x, y, theta)
theta <- thetaOld - alpha  * gradient
theta
sign(y_hat)
theta <- grad.descent(x = train_x,y = train_y,alpha = 0.001)
library(mlbench)
gcost<- function(x, y, theta) {
theta <- as.vector(theta)
gradient <- t(as.matrix(x)) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
return(gradient)
}
grad.descent <- function(x, y, maxit = 20000, alpha = 0.05, delta = 1e-8 ){
theta = runif(ncol(x)) # Initialize the parameters
#alpha = .05 # set learning rate
for (i in 1:maxit) {
alpha <- alpha * 0.98
thetaOld <- theta
gradient <- gcost(x, y, theta)
print(paste("iteration",i));
theta <- thetaOld - alpha  * gradient
#print(gradient)
if ( delta > max(abs(theta - thetaOld)) ){
print(paste("iteration",i));
print(theta)
return(theta)
}
}
return(theta)
}
data(BreastCancer)
# remove NA
df <- BreastCancer[complete.cases(BreastCancer),]
# data type convert for variables other than `ID` and `Class`
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df$label = rep(-1, nrow(df)) # create new column for label
df[df$Class=="malignant",]$label = 1 # malignant is 1, benign is -1
smp_size <- floor(0.70 * nrow(df))
# set the seed to make your partition reproductible
set.seed(1441)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
train_y <- train[,12]
train_x$intercept <- rep(1, nrow(train_x))
#train_x <- cbind(intercept,train_x)
# my code is not correct...
theta <- grad.descent(x = train_x,y = train_y,alpha = 0.001)
y_hat <- as.matrix(test[,2:10]) %*% theta[1:9]
test  <- cbind(test,y_hat)
test
theta <- grad.descent(x = train_x,y = train_y,alpha = 0.001)
y_hat <- sign(as.matrix(test[,2:10]) %*% theta[1:9])
test  <- df[-train_ind, ]
test  <- cbind(test,y_hat)
View(test)
p <- 1／exp(train_y *( as.matrix(train_x) %*% theta)))
p <- 1 / exp(train_y *( as.matrix(train_x) %*% theta)))
p <- 1 / (1+exp(train_y *( as.matrix(train_x) %*% theta)))
View(p)
View(train_y)
temp <-data.frame(train_y= train_y,p = p)
View(temp)
p <- 1 / (1+exp( as.matrix(train_x) %*% theta)))
p <- 1 / (1+exp( as.matrix(train_x) %*% theta))
temp <-data.frame(train_y= train_y,p = p)
?predict
View(train_x)
y_hat == train_y
y_hat
train_y
dim(train_y)
dim(y_hat)
gcost<- function(x, y, theta) {
theta <- as.vector(theta)
gradient <- t(as.matrix(x)) %*% (y / ( 1 + exp(-y *( as.matrix(x) %*% theta))))
return(gradient)
}
# define gradient descent update algorithm
# x: training sample vector,
# y: training sample output vector,
# alpha: learn rate scalar,
# delta: convergence delta limit scalar
# maxit: maximum number of iterations before convergence
LogitRegress <- function(x, y, maxit = 20000, alpha = 0.05, delta = 1e-8 ){
theta = runif(ncol(x)) # Initialize the parameters
#alpha = .05 # set learning rate
for (i in 1:maxit) {
alpha <- alpha * 0.98
thetaOld <- theta
gradient <- gcost(x, y, theta)
print(paste("iteration",i));
theta <- thetaOld - alpha  * gradient
#print(gradient)
if ( delta > max(abs(theta - thetaOld)) ){
print(paste("iteration",i));
print(theta)
return(theta)
}
}
return(theta)
}
PredictLR <- function(theta, X){
# Predict whether the label
# is 1 or -1 using learned logistic
# regression parameters
y_hat <- rep(-1,dim(X)[1])
p <- 1 / (1+exp( as.matrix(X) %*% theta))
y_hat[p > 0.5] <- 1
return(y_hat)
}
data(BreastCancer)
# remove NA
df <- BreastCancer[complete.cases(BreastCancer),]
# data type convert for variables other than `ID` and `Class`
df[,2:10] <- lapply(df[,2:10], function (x) as.numeric(levels(x))[x])
df$label = rep(-1, nrow(df)) # create new column for label
df[df$Class=="malignant",]$label = 1 # malignant is 1, benign is -1
smp_size <- floor(0.70 * nrow(df))
# set the seed to make your partition reproductible
set.seed(1441)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test  <- df[-train_ind, ]
train_x <- train[,2:10]
train_y <- train[,12]
test_x <- test[,2:10]
test_y <- test[,12]
train_x$intercept <- rep(1, nrow(train_x))
theta <- LogitRegress(x = train_x,y = train_y,alpha = 0.001)
y_hat <- PredictLR(X = as.matrix(train_x), theta = theta)
y_hat == train_y
accuracy <-  sum(y_hat == train_y) / length(train_y) * 100.0)
accuracy <-  sum(y_hat == train_y) / length(train_y) * 100.0
pwd
getwd()
df <- read.csv("/Users/Yang/GitHub/RunningStreak/runningRecords2016.csv")
summary(df)
df <- read.csv("/Users/Yang/GitHub/RunningStreak/runningRecords2016.csv")
summary(df)
df <- read.csv("/Users/Yang/GitHub/RunningStreak/runningRecords2016.csv")
summary(df)
table(df$weather)
df <- read.csv("/Users/Yang/GitHub/RunningStreak/runningRecords2016.csv")
table(df$weather)
df <- read.csv("/Users/Yang/GitHub/RunningStreak/runningRecords2016.csv")
table(df$weather)
View(df)
sum(df$Distance.mile.)
sum(df$runningTime.min.)
sum(df$runningTime.min.)/60
boxplot(df$runningTime.min.)
boxplot(df$temperature.C.)
plot(df$startTime,df$runningTime.min.)
plot(df$date,df$runningTime.min.)
plot(df$date,df$startTime)
df <- read.csv("../runningRecords2016.csv")
setwd("GitHub/RunningStreak/Analysis/")
df <- read.csv("../runningRecords2016.csv")
rowsum(df)
colSums(df)
class(df)
str(df)
colSums(df[,c(5,6,8)])
colMean(df[,c(5,6,8)])
colMeans(df[,c(5,6,8)])
7684.5／1010.4
7684.5／1010.4
7684.5/1010.4
1010.4/7684.5
table(df$weather)
sum(grepl(x=df$startTime, pattern = "am"))
sum(grepl(x=df$startTime, pattern = "pm"))
plot(x=df$date,grepl(x=df$startTime, pattern = "am"))
